{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "p8jVLhLJFz8S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7dbb3ad-13da-44c1-d854-98c7ec6f95ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai-agents\n",
            "  Downloading openai_agents-0.3.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting griffe<2,>=1.5.6 (from openai-agents)\n",
            "  Downloading griffe-1.14.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: mcp<2,>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from openai-agents) (1.16.0)\n",
            "Requirement already satisfied: openai<2,>=1.107.1 in /usr/local/lib/python3.12/dist-packages (from openai-agents) (1.109.1)\n",
            "Requirement already satisfied: pydantic<3,>=2.10 in /usr/local/lib/python3.12/dist-packages (from openai-agents) (2.11.10)\n",
            "Requirement already satisfied: requests<3,>=2.0 in /usr/local/lib/python3.12/dist-packages (from openai-agents) (2.32.4)\n",
            "Collecting types-requests<3,>=2.0 (from openai-agents)\n",
            "  Downloading types_requests-2.32.4.20250913-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from openai-agents) (4.15.0)\n",
            "Collecting colorama>=0.4 (from griffe<2,>=1.5.6->openai-agents)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: anyio>=4.5 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (4.11.0)\n",
            "Requirement already satisfied: httpx-sse>=0.4 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (0.4.2)\n",
            "Requirement already satisfied: httpx>=0.27.1 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (0.28.1)\n",
            "Requirement already satisfied: jsonschema>=4.20.0 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (4.25.1)\n",
            "Requirement already satisfied: pydantic-settings>=2.5.2 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (2.11.0)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (0.0.20)\n",
            "Requirement already satisfied: sse-starlette>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (3.0.2)\n",
            "Requirement already satisfied: starlette>=0.27 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (0.48.0)\n",
            "Requirement already satisfied: uvicorn>=0.31.1 in /usr/local/lib/python3.12/dist-packages (from mcp<2,>=1.11.0->openai-agents) (0.37.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2,>=1.107.1->openai-agents) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2,>=1.107.1->openai-agents) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2,>=1.107.1->openai-agents) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<2,>=1.107.1->openai-agents) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.10->openai-agents) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.10->openai-agents) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.10->openai-agents) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0->openai-agents) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0->openai-agents) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0->openai-agents) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0->openai-agents) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.1->mcp<2,>=1.11.0->openai-agents) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.1->mcp<2,>=1.11.0->openai-agents) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp<2,>=1.11.0->openai-agents) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp<2,>=1.11.0->openai-agents) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp<2,>=1.11.0->openai-agents) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp<2,>=1.11.0->openai-agents) (0.27.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings>=2.5.2->mcp<2,>=1.11.0->openai-agents) (1.1.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn>=0.31.1->mcp<2,>=1.11.0->openai-agents) (8.3.0)\n",
            "Downloading openai_agents-0.3.3-py3-none-any.whl (210 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.9/210.9 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading griffe-1.14.0-py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.4/144.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_requests-2.32.4.20250913-py3-none-any.whl (20 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: types-requests, colorama, griffe, openai-agents\n",
            "Successfully installed colorama-0.4.6 griffe-1.14.0 openai-agents-0.3.3 types-requests-2.32.4.20250913\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.11.10)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai-agents\n",
        "!pip install openai\n",
        "!pip install pydantic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pjlQPJ4UFs1u"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel\n",
        "from agents import Agent, ModelSettings, TResponseInputItem, Runner, RunConfig\n",
        "from openai.types.shared.reasoning import Reasoning\n",
        "\n",
        "class TriageRequestSchema(BaseModel):\n",
        "  classification: str\n",
        "\n",
        "\n",
        "class ApprovalAgentSchema(BaseModel):\n",
        "  emailFrom: str\n",
        "  defaultTo: str\n",
        "  defaultSubject: str\n",
        "  defaultBody: str\n",
        "\n",
        "\n",
        "triage_request = Agent(\n",
        "  name=\"Triage request\",\n",
        "  instructions=\"\"\"Classify the user's request based on whether two documents have been provided recently in the conversation, and whether the user is asking a particular question.\n",
        "\n",
        "If two documents are provided and there's no user question , respond with \\\"compare\\\".\n",
        "If two documents are provided and there is a user question , respond with \\\"answer_question\\\".\n",
        "If only one doc has been provided, or no docs have been provided, respond with \\\"request_upload\\\"\"\"\",\n",
        "  model=\"gpt-4.1\",\n",
        "  output_type=TriageRequestSchema,\n",
        "  model_settings=ModelSettings(\n",
        "    temperature=1,\n",
        "    top_p=1,\n",
        "    max_tokens=2048,\n",
        "    store=True\n",
        "  )\n",
        ")\n",
        "\n",
        "\n",
        "propose_reconciliation = Agent(\n",
        "  name=\"Propose reconciliation\",\n",
        "  instructions=\"Given the differences between the two documents, assemble a single option for how to reconcile the difference. If no order has been described, consider the first document the user's version and the second document the potential set of changes returned back to the user. The proposal you create will be sent to the user for approval.\",\n",
        "  model=\"gpt-5\",\n",
        "  model_settings=ModelSettings(\n",
        "    store=True,\n",
        "    reasoning=Reasoning(\n",
        "      effort=\"minimal\",\n",
        "      summary=\"auto\"\n",
        "    )\n",
        "  )\n",
        ")\n",
        "\n",
        "\n",
        "approval_agent = Agent(\n",
        "  name=\"Approval agent\",\n",
        "  instructions=\"\"\"Explain your approval reasoning. Help the user draft a proper response by filling out this data schema:\n",
        "\n",
        "{\n",
        "  emailFrom: 'user@test.com',\n",
        "  defaultTo: 'user@test.com',\n",
        "  defaultSubject: 'Document comparison proposal',\n",
        "  defaultBody: \\\"Hey there, \\n\\nHope you're doing well! Just wanted to check in and see if there are any updates on the ChatKit roadmap. We're excited to see what's coming next and how we can make the most of the upcoming features.\\n\\nEspecially curious to see how you support widgets!\\n\\nBest,\\\",\n",
        "}\"\"\",\n",
        "  model=\"gpt-5-mini\",\n",
        "  output_type=ApprovalAgentSchema,\n",
        "  model_settings=ModelSettings(\n",
        "    store=True,\n",
        "    reasoning=Reasoning(\n",
        "      effort=\"low\",\n",
        "      summary=\"auto\"\n",
        "    )\n",
        "  )\n",
        ")\n",
        "\n",
        "\n",
        "rejection_agent = Agent(\n",
        "  name=\"Rejection agent\",\n",
        "  instructions=\"Explain your rejection reasoning.\",\n",
        "  model=\"gpt-5\",\n",
        "  model_settings=ModelSettings(\n",
        "    store=True,\n",
        "    reasoning=Reasoning(\n",
        "      effort=\"low\",\n",
        "      summary=\"auto\"\n",
        "    )\n",
        "  )\n",
        ")\n",
        "\n",
        "\n",
        "retry_agent = Agent(\n",
        "  name=\"Retry agent\",\n",
        "  instructions=\"The user has not uploaded the required two documents for comparison. Suggest that they upload a total of two documents, using the paperclip icon.\",\n",
        "  model=\"gpt-5-nano\",\n",
        "  model_settings=ModelSettings(\n",
        "    store=True,\n",
        "    reasoning=Reasoning(\n",
        "      effort=\"minimal\",\n",
        "      summary=\"auto\"\n",
        "    )\n",
        "  )\n",
        ")\n",
        "\n",
        "\n",
        "provide_explanation = Agent(\n",
        "  name=\"Provide explanation\",\n",
        "  instructions=\"Use the information in the uploaded documents to answer the user's question.\",\n",
        "  model=\"gpt-5-nano\",\n",
        "  model_settings=ModelSettings(\n",
        "    store=True,\n",
        "    reasoning=Reasoning(\n",
        "      effort=\"minimal\",\n",
        "      summary=\"auto\"\n",
        "    )\n",
        "  )\n",
        ")\n",
        "\n",
        "\n",
        "def approval_request(message: str):\n",
        "  # TODO: Implement\n",
        "  return True\n",
        "\n",
        "class WorkflowInput(BaseModel):\n",
        "  input_as_text: str\n",
        "\n",
        "\n",
        "# Main code entrypoint\n",
        "async def run_workflow(workflow_input: WorkflowInput):\n",
        "  state = {\n",
        "\n",
        "  }\n",
        "  workflow = workflow_input.model_dump()\n",
        "  conversation_history: list[TResponseInputItem] = [\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"type\": \"input_text\",\n",
        "          \"text\": workflow[\"input_as_text\"]\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "  triage_request_result_temp = await Runner.run(\n",
        "    triage_request,\n",
        "    input=[\n",
        "      *conversation_history\n",
        "    ],\n",
        "    run_config=RunConfig(trace_metadata={\n",
        "      \"__trace_source__\": \"agent-builder\",\n",
        "      \"workflow_id\": \"wf_68e7d3ecffdc81909d6bd4ef54e13f97041f23f4d1d6d373\"\n",
        "    })\n",
        "  )\n",
        "\n",
        "  conversation_history.extend([item.to_input_item() for item in triage_request_result_temp.new_items])\n",
        "\n",
        "  triage_request_result = {\n",
        "    \"output_text\": triage_request_result_temp.final_output.json(),\n",
        "    \"output_parsed\": triage_request_result_temp.final_output.model_dump()\n",
        "  }\n",
        "  if triage_request_result[\"output_parsed\"][\"classification\"] == \"compare\":\n",
        "    propose_reconciliation_result_temp = await Runner.run(\n",
        "      propose_reconciliation,\n",
        "      input=[\n",
        "        *conversation_history\n",
        "      ],\n",
        "      run_config=RunConfig(trace_metadata={\n",
        "        \"__trace_source__\": \"agent-builder\",\n",
        "        \"workflow_id\": \"wf_68e7d3ecffdc81909d6bd4ef54e13f97041f23f4d1d6d373\"\n",
        "      })\n",
        "    )\n",
        "\n",
        "    conversation_history.extend([item.to_input_item() for item in propose_reconciliation_result_temp.new_items])\n",
        "\n",
        "    propose_reconciliation_result = {\n",
        "      \"output_text\": propose_reconciliation_result_temp.final_output_as(str)\n",
        "    }\n",
        "    approval_message = f\"Please review the proposal {propose_reconciliation_result[\"output_text\"]}\"\n",
        "\n",
        "    if approval_request(approval_message):\n",
        "        approval_agent_result_temp = await Runner.run(\n",
        "          approval_agent,\n",
        "          input=[\n",
        "            *conversation_history\n",
        "          ],\n",
        "          run_config=RunConfig(trace_metadata={\n",
        "            \"__trace_source__\": \"agent-builder\",\n",
        "            \"workflow_id\": \"wf_68e7d3ecffdc81909d6bd4ef54e13f97041f23f4d1d6d373\"\n",
        "          })\n",
        "        )\n",
        "\n",
        "        conversation_history.extend([item.to_input_item() for item in approval_agent_result_temp.new_items])\n",
        "\n",
        "        approval_agent_result = {\n",
        "          \"output_text\": approval_agent_result_temp.final_output.json(),\n",
        "          \"output_parsed\": approval_agent_result_temp.final_output.model_dump()\n",
        "        }\n",
        "    else:\n",
        "        rejection_agent_result_temp = await Runner.run(\n",
        "          rejection_agent,\n",
        "          input=[\n",
        "            *conversation_history\n",
        "          ],\n",
        "          run_config=RunConfig(trace_metadata={\n",
        "            \"__trace_source__\": \"agent-builder\",\n",
        "            \"workflow_id\": \"wf_68e7d3ecffdc81909d6bd4ef54e13f97041f23f4d1d6d373\"\n",
        "          })\n",
        "        )\n",
        "\n",
        "        conversation_history.extend([item.to_input_item() for item in rejection_agent_result_temp.new_items])\n",
        "\n",
        "        rejection_agent_result = {\n",
        "          \"output_text\": rejection_agent_result_temp.final_output_as(str)\n",
        "        }\n",
        "  elif triage_request_result[\"output_parsed\"][\"classification\"] == \"answer_question\":\n",
        "    provide_explanation_result_temp = await Runner.run(\n",
        "      provide_explanation,\n",
        "      input=[\n",
        "        *conversation_history\n",
        "      ],\n",
        "      run_config=RunConfig(trace_metadata={\n",
        "        \"__trace_source__\": \"agent-builder\",\n",
        "        \"workflow_id\": \"wf_68e7d3ecffdc81909d6bd4ef54e13f97041f23f4d1d6d373\"\n",
        "      })\n",
        "    )\n",
        "\n",
        "    conversation_history.extend([item.to_input_item() for item in provide_explanation_result_temp.new_items])\n",
        "\n",
        "    provide_explanation_result = {\n",
        "      \"output_text\": provide_explanation_result_temp.final_output_as(str)\n",
        "    }\n",
        "  else:\n",
        "    retry_agent_result_temp = await Runner.run(\n",
        "      retry_agent,\n",
        "      input=[\n",
        "        *conversation_history\n",
        "      ],\n",
        "      run_config=RunConfig(trace_metadata={\n",
        "        \"__trace_source__\": \"agent-builder\",\n",
        "        \"workflow_id\": \"wf_68e7d3ecffdc81909d6bd4ef54e13f97041f23f4d1d6d373\"\n",
        "      })\n",
        "    )\n",
        "\n",
        "    conversation_history.extend([item.to_input_item() for item in retry_agent_result_temp.new_items])\n",
        "\n",
        "    retry_agent_result = {\n",
        "      \"output_text\": retry_agent_result_temp.final_output_as(str)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyGrMGqsMsGW",
        "outputId": "f3d9d5e3-2bf5-4968-f659-b7cbff46e7a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected agents (execution order): ['approval_agent', 'propose_reconciliation', 'provide_explanation', 'rejection_agent', 'retry_agent', 'triage_request', 'web_research_agent', 'summarize_and_display']\n",
            "\n",
            "→ Running agent: 'Approval agent'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2870606196.py:86: PydanticDeprecatedSince20: The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  final_json = final_obj.json() if hasattr(final_obj, \"json\") else None\n",
            "Exception in thread Thread-10 (_runner):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1012, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/tmp/ipython-input-2870606196.py\", line 113, in _runner\n",
            "  File \"/usr/lib/python3.12/asyncio/runners.py\", line 195, in run\n",
            "    return runner.run(main)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n",
            "    return self._loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 691, in run_until_complete\n",
            "    return future.result()\n",
            "           ^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2870606196.py\", line 107, in _main\n",
            "AttributeError: 'RunResult' object has no attribute 'state'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Agent final output ===\n",
            "{\n",
            "  \"final_output_text\": \"{\\\"emailFrom\\\":\\\"user@test.com\\\",\\\"defaultTo\\\":\\\"user@test.com\\\",\\\"defaultSubject\\\":\\\"Document comparison proposal\\\",\\\"defaultBody\\\":\\\"Hi Éfrem,\\\\n\\\\nI reviewed the CV/summary you shared and I approve it for use/publication. Reasoning: the document is comprehensive and up to date (last updated 01/09/2025 per Lattes); it clearly lists education (PhD, MSc, BSc), current roles (Principal Product Manager at Pipefy; professor roles at FGV and Unieuro), relevant work history (Pipefy, Zeeplo, OLX, Globo, Loft, etc.), teaching and extension activities, research lines, publications, and contact information. Key strengths: clear product + AI focus, strong teaching experience, practical no-code/low-code expertise, and relevant publications and projects. \\\\n\\\\nSuggested minor edits before final distribution:\\\\n- Fix small typos/encoding issues (e.g., “Reponsável” -> \\\\\\\"Responsável\\\\\\\", remove duplicated punctuation or odd characters in headings).\\\\n- Standardize date formats and role ranges (e.g., ensure consistency between Lattes and LinkedIn dates where needed).\\\\n- Confirm telephone format (include +55 country code consistently) and preferred email for public listing (efrem@without.dev is shown).\\\\n- Verify links (efrem.io/cv and Lattes URL) open correctly and are the intended public pages.\\\\n- If you want a short one-page bio for websites or speaking engagements, I can produce a 3–5 sentence summary emphasizing Product, AI, and teaching expertise.\\\\n\\\\nNext steps: let me know if you want me to make the edits above and produce a polished one-page CV and a short bio/LinkedIn summary. If you prefer, I can also convert the CV into a clean PDF or a formatted LinkedIn/About page copy.\\\\n\\\\nBest,\\\\n[Your name]\\\"}\",\n",
            "  \"final_output_parsed\": {\n",
            "    \"emailFrom\": \"user@test.com\",\n",
            "    \"defaultTo\": \"user@test.com\",\n",
            "    \"defaultSubject\": \"Document comparison proposal\",\n",
            "    \"defaultBody\": \"Hi Éfrem,\\n\\nI reviewed the CV/summary you shared and I approve it for use/publication. Reasoning: the document is comprehensive and up to date (last updated 01/09/2025 per Lattes); it clearly lists education (PhD, MSc, BSc), current roles (Principal Product Manager at Pipefy; professor roles at FGV and Unieuro), relevant work history (Pipefy, Zeeplo, OLX, Globo, Loft, etc.), teaching and extension activities, research lines, publications, and contact information. Key strengths: clear product + AI focus, strong teaching experience, practical no-code/low-code expertise, and relevant publications and projects. \\n\\nSuggested minor edits before final distribution:\\n- Fix small typos/encoding issues (e.g., “Reponsável” -> \\\"Responsável\\\", remove duplicated punctuation or odd characters in headings).\\n- Standardize date formats and role ranges (e.g., ensure consistency between Lattes and LinkedIn dates where needed).\\n- Confirm telephone format (include +55 country code consistently) and preferred email for public listing (efrem@without.dev is shown).\\n- Verify links (efrem.io/cv and Lattes URL) open correctly and are the intended public pages.\\n- If you want a short one-page bio for websites or speaking engagements, I can produce a 3–5 sentence summary emphasizing Product, AI, and teaching expertise.\\n\\nNext steps: let me know if you want me to make the edits above and produce a polished one-page CV and a short bio/LinkedIn summary. If you prefer, I can also convert the CV into a clean PDF or a formatted LinkedIn/About page copy.\\n\\nBest,\\n[Your name]\"\n",
            "  },\n",
            "  \"trace_id\": null,\n",
            "  \"interruptions\": []\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import os, json, asyncio, threading\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from google.colab import userdata\n",
        "\n",
        "# -------- INPUTS (edit these) --------\n",
        "USER_TEXT: str = \"\"           # or \"\" if you only want to send files\n",
        "FILE_PATHS: List[str] = [\"/content/lattes.pdf\", \"/content/linkedin.pdf\"]          # e.g. [\"/sample.pdf\", \"/image.png\"]\n",
        "# -------------------------------------\n",
        "\n",
        "# 0) Check API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    raise RuntimeError(\"OPENAI_API_KEY is not set in the environment.\")\n",
        "\n",
        "# 1) Imports (Agents SDK)\n",
        "from openai import OpenAI\n",
        "from agents import Agent, Runner, RunConfig, TResponseInputItem\n",
        "\n",
        "# 2) Discover Agents defined in Cell 2 (generic)\n",
        "def _discover_agents() -> List[Tuple[str, Agent]]:\n",
        "    import __main__\n",
        "    agents = []\n",
        "    for name, val in vars(__main__).items():\n",
        "        if isinstance(val, Agent):\n",
        "            agents.append((name, val))\n",
        "    if not agents:\n",
        "        raise RuntimeError(\"No Agent instances found. Make sure Cell 2 created at least one Agent.\")\n",
        "    # Heuristic: place summarizers last\n",
        "    def _key(pair):\n",
        "        name, _ = pair\n",
        "        is_summarizer = int(any(k in name.lower() for k in (\"summary\",\"summarize\",\"display\",\"final\")))\n",
        "        return (is_summarizer, name.lower())\n",
        "    agents.sort(key=_key)\n",
        "    return agents\n",
        "\n",
        "# 3) Build Responses-style inputs (text + files)\n",
        "def _build_items(user_text: str, file_paths: List[str]) -> List[TResponseInputItem]:\n",
        "    content: List[Dict[str, Any]] = []\n",
        "    user_text = (user_text or \"\").strip()\n",
        "    if user_text:\n",
        "        content.append({\"type\": \"input_text\", \"text\": user_text})\n",
        "    # upload files -> input_file items\n",
        "    if file_paths:\n",
        "        client = OpenAI()\n",
        "        for p in file_paths:\n",
        "            path = Path(p).expanduser().resolve()\n",
        "            if not path.exists() or not path.is_file():\n",
        "                raise FileNotFoundError(f\"File not found: {path}\")\n",
        "            with path.open(\"rb\") as f:\n",
        "                up = client.files.create(file=(path.name, f.read()), purpose=\"assistants\")\n",
        "            content.append({\"type\": \"input_file\", \"file_id\": up.id})\n",
        "    if not content:\n",
        "        raise ValueError(\"Provide USER_TEXT and/or FILE_PATHS.\")\n",
        "    # one user message with all content\n",
        "    return [{\"role\": \"user\", \"content\": content}]\n",
        "\n",
        "# 4) Run one agent with approval loop\n",
        "async def _run_with_approvals(agent: Agent, input_payload):\n",
        "    print(f\"\\n→ Running agent: {agent.name!r}\")\n",
        "    result = await Runner.run(agent, input=input_payload, run_config=RunConfig())\n",
        "    # approvals loop\n",
        "    while True:\n",
        "        interruptions = getattr(result, \"interruptions\", []) or []\n",
        "        approvals = [intr for intr in interruptions\n",
        "                     if getattr(intr, \"type\", None) in (\"tool_approval_item\", \"mcp_approval_item\", \"approval_item\")]\n",
        "        if not approvals:\n",
        "            break\n",
        "        print(\"\\n=== Approvals requested ===\")\n",
        "        for idx, intr in enumerate(approvals, 1):\n",
        "            raw = getattr(intr, \"raw_item\", None)\n",
        "            tool_name = getattr(raw, \"name\", \"tool\")\n",
        "            tool_args = getattr(raw, \"arguments\", {})\n",
        "            print(f\"[{idx}] {tool_name} args={tool_args}\")\n",
        "            resp = input(\"Approve? [y/N]: \").strip().lower()\n",
        "            if resp in (\"y\",\"yes\"):\n",
        "                result.state.approve(intr)\n",
        "                print(\" → approved.\")\n",
        "            else:\n",
        "                result.state.reject(intr)\n",
        "                print(\" → rejected.\")\n",
        "        # resume with updated state\n",
        "        result = await Runner.run(agent, input=result.state, run_config=RunConfig())\n",
        "    # pretty output\n",
        "    final_obj = getattr(result, \"final_output\", None)\n",
        "    final_json = final_obj.json() if hasattr(final_obj, \"json\") else None\n",
        "    final_parsed = final_obj.model_dump() if hasattr(final_obj, \"model_dump\") else None\n",
        "    print(\"\\n=== Agent final output ===\")\n",
        "    print(json.dumps({\n",
        "        \"final_output_text\": final_json,\n",
        "        \"final_output_parsed\": final_parsed,\n",
        "        \"trace_id\": getattr(result, \"trace_id\", None),\n",
        "        \"interruptions\": [getattr(i, \"type\", None) for i in getattr(result, \"interruptions\", [])] if hasattr(result, \"interruptions\") else [],\n",
        "    }, ensure_ascii=False, indent=2))\n",
        "    return result\n",
        "\n",
        "# 5) End-to-end: run all discovered agents in sequence, passing the evolved state forward\n",
        "async def _main():\n",
        "    agents_list = _discover_agents()\n",
        "    print(\"Detected agents (execution order):\", [name for name,_ in agents_list])\n",
        "    items = _build_items(USER_TEXT, FILE_PATHS)\n",
        "\n",
        "    # first agent gets the user message; subsequent agents receive the evolved state\n",
        "    state_or_items = items\n",
        "    for name, agent in agents_list:\n",
        "        result = await _run_with_approvals(agent, state_or_items)\n",
        "        state_or_items = result.state  # pass the whole state forward\n",
        "\n",
        "    print(\"\\n✅ Workflow complete.\")\n",
        "\n",
        "# 6) Run in a dedicated thread with its own asyncio loop (avoids notebook loop conflicts)\n",
        "def _runner():\n",
        "    asyncio.run(_main())\n",
        "\n",
        "t = threading.Thread(target=_runner, daemon=False)\n",
        "t.start()\n",
        "t.join()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZtdT0X9v34Y6dbfW9bJOu"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}